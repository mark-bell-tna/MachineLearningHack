{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam filtering tutorial\n",
    "import os  # this library enables file system operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data and add body of text for each email into a list\n",
    "# The text starts on the third line\n",
    "# The filenames themselves begin with 'spm' if they are spam so use that to create classification labels\n",
    "# Once these commands have run we will have a list of email contents and a list of labels for the emails\n",
    "# to train our model with\n",
    "train_dir = 'Data/Spam/train-mails/'\n",
    "email_files = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "training_data = []\n",
    "train_labels = np.zeros(len(email_files))\n",
    "for n, mail in enumerate(email_files):\n",
    "    if mail.split('/')[3][0:3] == 'spm':\n",
    "        train_labels[n] = 1\n",
    "    with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    training_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions to convert text to a numerical matrix representation\n",
    "# Each row of the matrix will represent an email\n",
    "# Each column represents a word. The words are generated from the full set of emails.\n",
    "# The no_features variable is used to decide how many words to include in the model\n",
    "# This should be experimented with to find the best results\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we are using the TFIDF representation of the words\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "\n",
    "# Convert the text in documents to TFIDF\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(training_data)\n",
    "\n",
    "# This will return a list of the unique words used as features. Particularly useful if max_features has been used\n",
    "# because you can see which were retained.\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train two models to compare the outputs\n",
    "# Multinomial Naive Bayes takes a probabilistic approach\n",
    "# SVC uses a Support Vector Machine classifier which is a complex mathematical model\n",
    "model1 = MultinomialNB()\n",
    "model2 = LinearSVC()\n",
    "\n",
    "# Fit each model to the training data\n",
    "model1.fit(train_tfidf,train_labels)\n",
    "model2.fit(train_tfidf,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data and add body of text for each email into a list\n",
    "# Produce list of class labels too\n",
    "test_dir = 'Data/Spam/test-mails/'\n",
    "email_files = [os.path.join(test_dir,f) for f in os.listdir(test_dir)]\n",
    "test_data = []\n",
    "test_labels = np.zeros(len(email_files))\n",
    "for n,mail in enumerate(email_files):\n",
    "    if mail.split('/')[3][0:3] == 'spm':\n",
    "        test_labels[n] = 1\n",
    "    with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is only 3rd line of text file\n",
    "                    test_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes Results:\n",
      " [[128   2]\n",
      " [  8 122]] \n",
      "\n",
      "Support Vector Machine Results:\n",
      " [[129   1]\n",
      " [  4 126]]\n"
     ]
    }
   ],
   "source": [
    "# A confusion matrix is a useful method of checking the effectiveness of classification models\n",
    "# If we have a matrix C then for binary classification (indexing = C[row, column]):\n",
    "#     The count of true negatives is C[0,0]\n",
    "#     The count of false negatives is C[1,0]\n",
    "#     The count of true positives is C[1,1]\n",
    "#     The count of false positives is C[0,1].\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "result1 = model1.predict(test_tfidf)\n",
    "result2 = model2.predict(test_tfidf)\n",
    "print (\"NaiveBayes Results:\\n\",confusion_matrix(test_labels,result1), \"\\n\")\n",
    "print (\"Support Vector Machine Results:\\n\",confusion_matrix(test_labels,result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[129   1]\n",
      " [  6 124]]\n"
     ]
    }
   ],
   "source": [
    "# We can reduce the amount of typing a lot by using the pipeline function\n",
    "# It was useful above to break it into steps for learning purposes\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "pipeline.fit(training_data, train_labels)\n",
    "predictions = pipeline.predict(test_data) # ['spam', 'ham']\n",
    "print (\"Pipeline Results:\\n\",confusion_matrix(test_labels,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 702\n",
      "Score: 0.986266897092\n",
      "Confusion matrix:\n",
      " [[344   7]\n",
      " [  3 348]]\n"
     ]
    }
   ],
   "source": [
    "# Cross validation is an important technique for avoid overfitting a trained model\n",
    "# Overfitting means that it will work very well for a training set because the model has been tuned very\n",
    "# finely to get good results for it. But when we try it on new data it will perform badly.\n",
    "# Cross validation runs the pipeline several times with different training and validation sets\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "k_fold = KFold(n=len(training_data), n_folds=6)\n",
    "scores = []\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = [training_data[i] for i in train_indices]\n",
    "    train_y = train_labels[train_indices]\n",
    "\n",
    "    test_text = [training_data[i] for i in test_indices]\n",
    "    test_y = train_labels[test_indices]\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Total emails classified:', len(training_data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:\\n', confusion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
